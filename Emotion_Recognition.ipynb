{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kd1wUizkoQY",
        "outputId": "8130920f-05bc-4751-c527-6414c68cf853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'emotion_recognition'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Total 29 (delta 0), reused 0 (delta 0), pack-reused 29\u001b[K\n",
            "Unpacking objects: 100% (29/29), done.\n",
            "/content/emotion_recognition\n"
          ]
        }
      ],
      "source": [
        "### IMPORTANDO ARQUIVOS DO GITHUB ###\n",
        "!git clone https://github.com/pauloschmidt8/emotion_recognition.git\n",
        "### PASSANDO O REPOSITÓRIO\n",
        "%cd emotion_recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPORTANDO BIBLIOTECAS**"
      ],
      "metadata": {
        "id": "BEr9UxZwulS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "mode = \"display\"\n",
        "\n",
        "# CRIANDO O MODELO\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "\n",
        "def emotion_recog(frame):\n",
        "    model.load_weights('model.h5')\n",
        "\n",
        "    # prevents openCL usage and unnecessary logging messages\n",
        "    cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "    # EMOÇÕES \n",
        "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "    # frame = cv2.imread(\"image1.jpg\")\n",
        "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 255), 3)\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "        prediction = model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    # cv2_imshow(frame)\n",
        "    return frame"
      ],
      "metadata": {
        "id": "DdLXuhx9kzVZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BUSCA A IMAGEM E FAZ A ANALISE\n",
        "input = cv2.imread(\"feliz.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "4NDFFxnztY7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv2.imread(\"faces.jpg\")\n",
        "output = emotion_recog(input)\n",
        "cv2_imshow(output)"
      ],
      "metadata": {
        "id": "XB_0aWd6tcob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}